ğŸ§  Ollama-GPT â€” Self-Hosted Private LLM with Open WebUI
Run your own ChatGPT-like interface entirely offline using:

Ollama â€” Run AI models locally on your own machine.
Open WebUI â€” Beautiful ChatGPT-style interface.
âš¡ Privacy-first: No API keys, no data sharing â€” 100% offline.

ğŸ“¦ Features
ğŸš€ Run open-source LLMs locally (LLaMA, CodeLLaMA, Qwen, Phi, etc.)
ğŸ”’ Fully private â€” your data stays on your machine
ğŸ³ Docker-based setup for quick deployment
ğŸ’» CLI or Web UI access
âš™ Configurable CPU & RAM limits
ğŸš€ Setup Instructions
1ï¸âƒ£ Clone the Repository
git clone https://github.com/ashubambal/ollama-gpt.git
cd ollama-gpt
2ï¸âƒ£ Configure Environment Variables (Optional)
Edit docker-compose.yml and set your own WEBUI_SECRET_KEY:

WEBUI_SECRET_KEY=enter-your-secret-key
3ï¸âƒ£ Start the Containers
docker compose up -d
4ï¸âƒ£ Verify Running Containers
docker ps
Expected:

ollama â€” AI model server
open-webui â€” Web interface
5ï¸âƒ£ Install a Model in Ollama
Enter the Ollama container:
docker exec -it ollama bash
Pull a model (example: llama3.2:1b-instruct-q4_0):
ollama pull llama3.2:1b-instruct-q4_0
Test in CLI:
ollama run llama3.2:1b-instruct-q4_0
6ï¸âƒ£ Access via Web Interface
Open browser â†’ http://localhost:8080
Login using WEBUI_SECRET_KEY
Select your model and start chatting ğŸ‰
ğŸ“š Model Resources
Ollama Model Library
Recommended code-focused models:
codellama
qwen2.5-coder
phi3.5
ğŸ¥ Output Demo
Demo

ğŸ›  Stopping the Services
docker compose down
ğŸ“„ License
MIT â€” Free to use and modify.

 
This is **one continuous README** â€” no separated instructions, everything from install to demo is inside.  

If you want, I can now **add screenshots of the Open WebUI UI** directly into this same README so that new users see exactly what theyâ€™ll get before running it.  

Do you want me to add those screenshots in this file?